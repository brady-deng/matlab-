{
  "1":"
  排序算法
  插入排序，每次选择一个元素，并且将这个元素和整个数组中的所有元素进行比较，然后插入到合
  适的位置，时间复杂度O(n^2)

  希尔排序(Shell Sort)，这个是插入排序的修改版，根据步长由长到短分组进行排序，直到步长
  为1为止，希尔排序是一种不稳定的排序算法，对希尔排序的时间复杂度分析很困难

  基数排序(Radix Sort)，基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成
  不同的数字，然后按每个位数分别比较。排序过程是将所有待比较数值统一为同样的数位长度，数
  位较短的数前面补零，然后从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排
  序完成以后，数列就变成一个有序数列。在某些时候，基数排序法的效率高于其它的稳定性排序法
  每次选择两个元素，按照需求进行交换，循环n次(n为总元素个数)，这样小的元素会不断“冒泡”到
  前面来，时间复杂度为O(n^2)

  归并排序(Merge Sort)，归并排序相比较之前的排序算法而言加入了分治法的思想，其算法思路
  如下：
  1. 如果给的数组只有一个元素的话，直接返回(也就是递归到最底层的一个情况)
  2. 把整个数组分为尽可能相等的两个部分
  3. 对于两个被分开的两个部分进行整个归并排序
  4. 把两个被分开且排好序的数组拼接在一起
  算法复杂度nlogn

  堆排序(Heap Sort)，

  桶排序(Bucket Sort)

  快速排序(Quick Sort)

  Bogo排序



  ",
  "2":"
  堆栈
  是一种数据结构，只能在一端对数据项进行插入和删除，先进后出
  栈，由操作系统自动分配释放，存放函数的参数值、局部变量的值
  堆，一般由程序员分配释放
  ",
  "3":"
  等价类
  设R是定义在集合A上的等价关系，与A中一个元素a有关系的所有元素的集合叫做a的等价类
  有效等价类
  是指对于程序的规格说明来说是合理的、有意义的输入数据构成的集合。利用有效等价类可检验
  程序是否实现了规格说明所规定的功能和性能
  无效等价类
  指对程序的规格说明是不合理的或无意义的输入数据所构成的集合，对于具体的问题，无效等价类
  至少应有一个，也可能多个。
  ",
  "4":"
  spearman相关系数，它是衡量两个变量的依赖性的非参数指标，表示独立变量和依赖变量的相关
  方向。如果x增加时，y趋向于增加，spearman系数为正，x增加时，y趋向于减少，spearman系数
  为负。定序变量。
  ",
  "5":"
  散列表(Hash table, 也叫哈希表)，是根据关键码值(Key value)而直接进行访问的数据结构，
  也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数
  叫做散列函数，存放记录的数组叫做散列表。
  ",
  "6":"
  AR模型是一种线性预测模型，即已知N个数据，可由模型推出第N点前面或后面的数据
  MA模型(moving average model)滑动平均模型，模型参量法谱分析方法之一
  ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率
  分析方法之一。是研究平稳随机过程有理谱的典型方法，比上述两种有较优良的谱分辨率性能，但
  其参数估算比较繁琐
  GARCH模型，称为广义ARCH模型，适用于波动性的分析和预测。
  ",
  "7":"
  卡特兰数又称卡塔兰数，Catalan number
  从2n位二进制数中填入n个1的方案数为c(2n,n)，不填1的其余n位自动填0。从中减去不符合要求
  （从左而右扫描，0的累计数大于1的累计数）的方案数即为所求
  C(n+1) = C(0)C(n) + C(1)C(n-1) + ... + C(n)C(0)
  (n-3)C(n) = (n/2)(C(3)C(n-1) + C(4)C(n-2) + C(5)C(n-3) + ... + C(n-2)C(4) + C(n-1)C(3))
  h(n) = C(2n,n)/(n+1) (n=0,1,2,...)
  h(n) = C(2n,n) - C(2n,n-1) (n=0,1,2,...)
  0 1
  1 1
  ",
  "8":"
  基尼系数，分问题中假设有K个类，样本点属于第K个类的概率为pk，则概率分布的基尼指数定义为
  Gini(p) = 求和(pk(1-pk)) = 1 - 求和(pk**2)
  特性
  类别个数越少，基尼系数越低
  类别个数相同时，类别集中度越高，基尼系数越低
  当类别越少，类别集中度越高的时候，基尼系数越低；当类别越多，类别集中度越低的时候，基尼系数
  越高
  熵H(X) = -求和(pi*log(pi))
  特性
  非负性，信息量为正值
  对称性，关于P=0.5对称
  确定性，0或1时是确定状态
  极值性，0.5时取到极值-
  ",
  "线性回归":"
  线性回归的基本假设：
  1. 随机误差项是一个期望值或平均值为0的随机变量
  2. 对于解释变量的所有观测值，随机误差项有相同的方差
  3. 随机误差项彼此不相关
  4. 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立
  5. 解释变量之间不存在精确的(完全的)线性关系，即解释变量的样本观测值矩阵是满秩矩阵
  6. 随机误差项服从正态分布

  当存在异方差时，普通最小二乘法估计存在以下问题：参数估计值虽然是无偏的，但不是最小方差
  线性无偏估计

  杜宾-瓦特森(DW)检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。

  所谓多重共线性(Multicollinearity)是指线性回归模型中的解释变量之间由于存在精确相关关系
  或高度相关关系而使模型估计失真或难以估计准确。影响如下
  1. 完全共线性下参数估计量不存在
  2. 近似共线性下OLS估计量非有效，多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀
  因子
  3. 参数估计量经济含义不合理
  4. 变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外
  5. 模型的预测功能失效。变大的方差容易是区间预测的“区间”变大，使预测失去意义

  ",
  "LASSO 与 Ridge":"
  LASSO通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，即强制系数
  绝对值之和小于某个固定值；同时设定一些回归系数为零。因此保留了子集收缩的优点，是一种
  处理具有复共线性数据的有偏估计

  ",
  "PDF,PMF,CDF":"
  概率质量函数(probability mass function,PMF)是离散随机变量在各特定取值上的概率
  概率密度函数(probability density function,PDF)是对连续随机变量定义的，本身不是概率
  只有对连续随机变量的取值进行积分后才是概率
  累积分布函数(cumulative distribution function,CDF)能完整描述一个实数随机变量X的概率
  分布，是概率密度函数的积分
  ",
  "分词方法":"
  1. 基于语法和规则的分词法
  2. 机械式分词法(基于词典)：最大匹配法（正向、逆向）、最少切分法（使一句中切分出的词数
  最小，这也是基于词典分词的方法）
  3. 基于统计的方法，根据字符串在语料库中出现的统计频率来决定其是否构成词。条件随机场是
  一个基于统计的序列标记和分割的方法，定义了整个标签序列的联合概率。

  ",
  "序列标注":"
  隐马尔可夫模型(Hidden Markov Model, HMM)
  最大熵马尔可夫模型(Maximum Entropy Markov Model, MEMM)
  条件随机场(Conditional Random Field, CRF)
  HMM，对转移概率和表现概率直接建模，统计共现概率
  MEMM，对转移概率和表现概率建立联合概率，统计时统计的是条件概率，容易陷入局部最优
  CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，计算全局最优输出节点
  的条件概率，特征设计灵活，同时需要训练的参数更多，存在训练代价大、复杂度高的缺点。
  ",
  "特征提取":"
  K-L变换是除了PCA外的另一种常用的特征提取方法，它有很多种形式，最基本的形式跟PCA类似，
  它跟PCA的不同在于，PCA是一种无监督的特征变换，而K-L变换能够考虑到不同的分类信息，实现
  有监督的特征提取
  K-L变换的几个重要性质
  1. 变换后得到的新特征满足零均值
  2. K-L变换是一种正交变换
  3. K-L变换的新特征彼此之间不相关
  4. K-L变换的新特征向量的二阶矩阵是对角阵，且对角线元素就是原特征的二阶矩阵的特征值
  5. K-L变换是信号的最佳压缩表示，用q维新特征表示原样本特征带来的误差在所有q维正交坐标
  变换中最小
  6. 用K-L坐标系来表示元数据，意味着熵最小，即样本的方差信息最大程度的集中在较少的维数上


  K-L变换与PCA的联系与区别
  都属于正交变换
  当对原特征x进行中心化时(即变换矩阵为协方差矩阵)，K-L变换等价于PCA
  PCA是离散K-L变换

  K-L变换可以实现有监督的特征提取，但是PCA的变换是一种无监督的
  在含以上，K-L变换较广义，PCA较狭义
  K-L变换可以处理连续和离散情况，而PCA只针对离散情况
  K-L变换的变换矩阵可以是很多种，如二阶矩阵、协方差矩阵等，而PCA的变换矩阵就是协方差矩
  ",
  	"1":"
  	产生式模型(Generative Model)，对于输入x，类别标签y，估计它们的联合概率
  	分布P(x,y)
  	判别式模型估计条件概率分布P(y|x)
  	联系与区别
  	产生式模型可以根据贝叶斯公式得到判别式模型，但反过来不行
  	产生式模型，关注数据是如何生成的
  	判别式模型，关注类别之间的差别
  	常见的判别式模型:
  	Logistic Regression
  	SVM(SVM本身对噪声具有一定的鲁棒性，但实验证明，当噪声率低于一定水平的噪
  	声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低
  	Traditional Neural Networks
  	Nearest Neighbor
  	CRF(Conditional random field, 条件随机场)
  	Linear Discriminant Analysis
  	Boosting
  	Linear Regression
  	常见的产生式模型:
  	Gaussians(混合高斯模型): 混合高斯模型使用K个高斯模型来表征图像中各个像素
  	点的特征，在新一帧图像获得后更新混合高斯模型，用当前图像中的每个像素点与
  	混合高斯模型匹配，如果成功则判定该点为背景点，否则为前景点，主要由方差和
  	均值两个参数决定。
  	Naive Bayes
  	Mixtures of Gaussians
  	Mixtures of Experts
  	HMMs
  	Sigmoidal Belief Networks
  	Bayesian Networks
  	",
  	"2":"
  	置信度：同时购买了商品A和商品B的交易次数/购买了商品A的次数
  	支持度：同时购买了商品A和商品B 的交易次数/总的交易次数
  	",
  	"3":"
  	设A是数域上一个n阶矩阵，若在相同数域上存在另一个n阶矩阵B，使得AB=BA=E
  	则称B是A的逆矩阵
  	伪逆矩阵是逆矩阵的广义形式
  	奇异矩阵的秩不是满秩。
  	可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵

  	伪逆法：径向基(RBF)神经网络的训练算法，解决线性不可分的情况
  	感知器算法：线性分类模型
  	H-K算法：在最小均方误差准则下求得权矢量，二次准则解决非线性问题
  	势函数法：解决非线性问题
  	",
  	"4":"
  	前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率
  	用于评估该序列最匹配的模型
  	Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方
  	法，主要通过EM法迭代实现，用在只有观测序列，无状态序列时
  	维特比算法解决的是给定一个模型和某个特定的输出序列，求最可能产生这个输出
  	的状态序列。
  	极大似然估计，观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数
  	",
  	"5":"
  	在贝斯决策中，对于先验概率p(y)，分为已知和未知两种情况
  	p(y)已知，直接使用贝叶斯公式求后验概率即可
  	p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面
  	而最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的
  	问题
  	",
  	"6":"
  	系数自编码就是用少于输入层神经元数量的隐含层神经元去学习表征输入层的特征
  	相当于把输入层的特征压缩了，所以是特征降维
  	AutoEncoder的结构与神经网络的隐含层相同，由输入L1，输出L2组成，中间则是
  	权重链接，通过L2得到输入的重构L3，最小化L3与L1的差别进行训练得到权重，尽
  	可能的保存L1的信息。
  	AutoEncoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则
  	需要在训练目标函数中加入sparse惩罚项，避免L2直接复制L1（权重全为1），所以
  	称为sparseAutoencoder。

  	LDA(线性判别式)与PCA
  	PCA是一种无监督的数据降维方法，LDA是一种有监督的数据降维方法。PCA和LDA
  	都是将数据投影到新的相互正交的坐标轴上。PCA是将数据投影到方差最大的几个相
  	互正交的方向上，以期待保留最多的样本信息。
  	LDA降维根据以下三个条件
  	1. 尽可能多地保留数据样本的信息
  	2. 寻找是样本尽可能好分的最佳投影方向
  	3. 投影后使得同类样本尽可能近，不同类样本尽可能远
  	",
  	"7":"
  	卷积神经网络计算输出尺寸，卷积层向下取整，弛化层向上取整。
  	",
  	"8":"
  	Fisher准则，根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的
  	法线，向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分
  	开。这种度量通过类内离散矩阵Sw和类间离散矩阵Sb实现
  	感知准则函数，准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通
  	过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感
  	知器的基础
  	支持向量机，基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的
  	间隔为最大，它的基本出发点是使期望泛华风险尽可能小。
  	",
  	"9":"
  	信息增益计算公式，1-信息熵
  	",
  	"10":"
  	logistic回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比
  	与先验概率和似然函数的乘积，logistic回归的输出就是样本属于正类别的几率。
  	是一种广义的线性回归分析模型。
  	logistic函数就是sigmoid函数
  	",
  	"11":"
  	L1正则化/Lasso
  	L1正则化将系数w的1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那
  	些弱的特征所对应的系数变为0，因此L1正则化往往会使学到的模型很稀疏(系数w经常
  	为0)，这个特性使得L1正则化称为一种很好的特征选择方法。
  	L2正则化/Ridge regression
  	L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，
  	这使得L2和L1有着诸多的差异，最明显的一点就是，L2正则化会让系数的取值变得平
  	均。对于关联特征，这意味他们能够获得更相近的对应系数。
  	L2正则化对于特征选择来说是一种稳定的模型，不像L1正则化那样，系数会因为细微
  	的数据变化而波动，所以L2正则化和L1正则化提供的价值不同，L2正则化对于特征理
  	解来说更加有用。
  	",
  	"12":"
  	HK算法思想适用于线性可分和非线性可分的情况，对于线性可分的情况给出最优权矢
  	量，对于非线性可分的情况能够判别出来，以退出迭代过程。
  	",
  	"英文名词":"
  	径向基, Radial basis function
  	EM算法, Expectation maximization
  	",
  	"13":"
  	SGD, stochastic gradient descent, 随机梯度下降
  	BGD, batch gradient descent, 批量梯度下降
  	Adadelta, 自适应学习率调整
  	Momentum,
  	",
  	"14":"
  	黑塞矩阵(Hessian Matrix)，是一个多元函数的二阶偏导数构成的方阵，描述了
  	函数的局部曲率，可以判定多远函数的极值问题。
  	设n元实函数f(x1,x2,...,xn)在点M0(a1,a2,...,an)的邻域内有二阶连续偏导，A为
  	偏导矩阵
  	当A正定矩阵时，函数在该点为极小值
  	负定矩阵，在该点为极大值
  	不定矩阵，不是极值点
  	半正定矩阵或半负定矩阵，可疑极值点

  	埃尔米特矩阵是共轭对称的方阵，矩阵中每一个第i行j列的元素都与第j行i列的元素
  	共轭相等，是实对称矩阵的推广，如果它既不是半正定也不是半负定的，那么称其
  	为不定矩阵

  	鞍点(Saddle point)，在微分方程中沿着某一方向是稳定的，另一条方向是不稳定
  	的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍
  	点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值。

  	正定矩阵，正定矩阵是一种实对称矩阵，对任何非零向量z，都有zT*M*z>0
  	如果是≥0则称为半正定
  	",
  	"15":"
  	克莱姆法则适用于变量和方程数目相等的线性方程组
  	对于矩阵方程AX = B
  	A为系数矩阵
  	B为常数项列向量
  	当B全为零的时候为齐次线性方程组
  	当B不全为零的时候为非齐次线性方程组
  	1. 若系数矩阵可逆(非奇异)，即|A|不等于0，有唯一解，D=A
  	X0 = A(-1)*B
  	xj = Dj/D (j = 1,2,...,n)
  	其中Dj是把D中第j列元素对应地换成常数项而其余各列保持不变所得到的行列式
  	2. n元齐次线性方程组有非零解的充要条件是其系数行列式为零。等价地，方程
  	组有唯一的零解的充要条件是系数矩阵的行列式不为零，其矩阵可逆
  	3. 当方程组的系数行列式不等于零时，方程组有解，且具有唯一的解
  	4. 如果方程组无解或者有两个不同的解，那么方程组的系数行列式必定等于零
  	5. 克莱姆法则不仅仅适用于实数域，它在任何域上面都成立
  	",
  	"16":"
  	快速排序(Quick Sort)
  	时间复杂度并不固定，如果在最坏情况下（元素刚好是反向的）速度比较慢，达到
  	O(n^2)，但是如果在比较理想的情况下时间复杂度为O(nlogn)。
  	元素的选择一般有如下几种
  	永远选择第一个元素
  	永远选择最后一个元素
  	随机选择元素
  	取中间值
  	整个快速排序的核心是分区(paritition)，分区的目的是传入一个数组和选定的一个
  	元素，把所有小于那个元素的其他元素放在左边，大于的放在右边。

  	冒泡排序
  	比较相邻的元素，如果第一个比第二个大，就交换他们两个
  	对每一组相邻元素做同样的工作，从开始第一对到结尾的最后一对
  	针对所有的元素重复以上的步骤，除了第一个
  	持续每次对越来越少的元素重复上面的步骤，知道没有任何一对数字需要比较
  	",
  	"17":"
  	切比雪夫不等式可以使人们在随机变量x分布未知的情况下对事件|X-μ|<e概率做出
  	估计
  	依概率收敛是不确定现象中关于收敛的一种说法
  	Kolmogorov-Smirnov检验(K-S检验)基于累积分布函数，用以检验一个经验分布是
  	否符合某种理论分布或比较两个经验分布是否有显著性差异。
  	T检验，亦称student检验，主要用于样本含量较小，总体标准差δ未知的正态分布，
  	比较两个平均数的差异是否显著，与f检验、卡方检验并列。
  	Wilcoxon符号秩检验，把观测值和零假设的中心位置之差的绝对值的秩分别按照不同
  	的符号相加作为检验统计量，检测成对观测数据之差是否来自均值为0的总体(产生数据
  	的总体是否具有相同的均值)。
  	Q-Q图，用图形的方式比较两个概率分布，把他们的两个分位数放在一起比较，Q代表
  	分位数，如果两个分布相似，则该Q-Q图趋近于落在y=x的线上，要利用QQ图鉴别样本
  	数据是否近似于正态分布，只需看QQ图上的点是否近似地在一条直线附近，而且该直线
  	的斜率为标准差，截距为均值
  	分布的X2拟合优度检验也可以用来检验观测数据是否符合某种分布。
  	对所考察总体中每一个元素同时测定两个指标X，Y，要检验这两个指标是否有关采用
  	列联表的独立性检验
  	",
  	"18":"
  	中心极限定理，是指概率论中讨论随机变量序列部分和分布逐渐近于正态分布的一类
  	定理，伯努利试验中，事件A出现的次数渐进于正态分布的问题。在实际工作中，只要
  	n足够大，便可以把独立同分布的随机变量之和当做正态变量（一个随机变量，以概率
  	取值，它的取值服从正态分布）。
  	大数定律，在随机事件的大量重复出现中，往往呈现几乎必然的规律，这个规律就是
  	大数定律。在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。
  	切比雪夫大数定理
  	伯努利大数定律
  	辛钦大数定律
  	",
  	"19":"
  	偏差(Bias)，描述的是预测值的期望与真实值之间的差距。偏差越大，越偏离真实数据
  	形容数据跟我们期望的中心差得多远，算是有监督的

  	方差(Variance)，描述的是预测值的变化范围，离散程度，也就是其离期望值的距离。
  	方差越大，数据的分布越分散
  	形容数据的分散程度，算是无监督的

  	噪声(noise)，表达了在当前任务上任何学习算法所能达到的期望泛华误差的下届，刻画了
  	学习问题本身的难度

  	对于给定的学习任务，为了取得较好的泛华性能，需使偏差较小，能够充分拟合数据，
  	并且使方差较小，即使得数据扰动产生的影响小。

  	一般来说偏差与方差是有冲突的，这称为偏差-方差窘境，在训练不足时，学习器的拟合能力
  	不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛华错误率；随着
  	训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学习到，
  	方差逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练数据发
  	生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学
  	到了，则将发生过拟合。
  	",
  	"20":"
  	显著性检验的一般步骤
  	1. 提出假设, 根据问题的要求建立原假设H0和对立假设H1
  	2. 选统计量, 根据H0的内容选取一个合适的检验统计量T，确定它的抽样分布，算出抽样分布
  	的分位数
  	3. 给定显著性水平α的值, α一般取得比较小，如0.01，0.05，0.10等
  	4. 确定拒绝域, 在原假设H0正确的条件下，求出能使
  		Pr{(X1,X2,...,Xn)属于W|H0 } ≤ α
  	成立的拒绝域W，拒绝域W与所选的统计量T和假设有关，常用检验统计量T和相应的临界值c
  	表示
  	5. 对H0作出判断, 比较检验统计量T的观测值t和相应的临界值c，看样本观测值是否落入拒绝域
  	如果样本观测值(x1,x2,...,xn)属于W，则拒绝原假设H0，也称为检验结果是显著的；否则不拒绝
  	H0，或称为检验结果是不显著的
  	",
  	"21":"
  	分箱问题
  	无监督分箱
  	等宽分箱
  	将变量的取值范围分为k个等宽的区间，每个区间当做一个分箱
  	等频（等深）分箱
  	将观测值按照从小到大的顺序排列，根据观测的个数等分为k部分，每部分当做一个分箱
  	k聚类分箱
  	用k均值聚类法将观测值聚为k类，但在聚类过程中需要保证分箱的有序性。
  	有监督分箱
  	在分箱时考虑因变量的取值，使得分箱后达到最小或最小描述长度(总熵值达到最小)
  	",
  	"22":"
  	数据属性类型
  	1. 标称属性
  	2. 二元属性
  	3. 序数属性
  	4. 数值属性(区间标度属性与比率标度属性)
  	5. 离散属性与连续属性
  	",
  	"23":"
  	截断均值指的是指定0和100之间的百分数p，丢弃高端和低端(p/2)%的数据，然后用常规的方法计算
  	均值，所得的结果就是截断均值",
  	"24":"
  	规范化数据的3种方法
  	最小-最大规范化
  	对原始数据进行线性变换
  	z分数规范化
  	小数定标规范化
  	",
    "1":"
    排序算法
    插入排序，每次选择一个元素，并且将这个元素和整个数组中的所有元素进行比较，然后插入到合
    适的位置，时间复杂度O(n^2)

    希尔排序(Shell Sort)，这个是插入排序的修改版，根据步长由长到短分组进行排序，直到步长
    为1为止，希尔排序是一种不稳定的排序算法，对希尔排序的时间复杂度分析很困难

    基数排序(Radix Sort)，基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成
    不同的数字，然后按每个位数分别比较。排序过程是将所有待比较数值统一为同样的数位长度，数
    位较短的数前面补零，然后从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排
    序完成以后，数列就变成一个有序数列。在某些时候，基数排序法的效率高于其它的稳定性排序法
    每次选择两个元素，按照需求进行交换，循环n次(n为总元素个数)，这样小的元素会不断“冒泡”到
    前面来，时间复杂度为O(n^2)

    归并排序(Merge Sort)，归并排序相比较之前的排序算法而言加入了分治法的思想，其算法思路
    如下：
    1. 如果给的数组只有一个元素的话，直接返回(也就是递归到最底层的一个情况)
    2. 把整个数组分为尽可能相等的两个部分
    3. 对于两个被分开的两个部分进行整个归并排序
    4. 把两个被分开且排好序的数组拼接在一起
    算法复杂度nlogn

    堆排序(Heap Sort)，

    桶排序(Bucket Sort)

    快速排序(Quick Sort)

    Bogo排序



    ",
    "2":"
    堆栈
    是一种数据结构，只能在一端对数据项进行插入和删除，先进后出
    栈，由操作系统自动分配释放，存放函数的参数值、局部变量的值
    堆，一般由程序员分配释放
    ",
    "3":"
    等价类
    设R是定义在集合A上的等价关系，与A中一个元素a有关系的所有元素的集合叫做a的等价类
    有效等价类
    是指对于程序的规格说明来说是合理的、有意义的输入数据构成的集合。利用有效等价类可检验
    程序是否实现了规格说明所规定的功能和性能
    无效等价类
    指对程序的规格说明是不合理的或无意义的输入数据所构成的集合，对于具体的问题，无效等价类
    至少应有一个，也可能多个。
    ",
    "4":"
    spearman相关系数，它是衡量两个变量的依赖性的非参数指标，表示独立变量和依赖变量的相关
    方向。如果x增加时，y趋向于增加，spearman系数为正，x增加时，y趋向于减少，spearman系数
    为负。定序变量。
    ",
    "5":"
    散列表(Hash table, 也叫哈希表)，是根据关键码值(Key value)而直接进行访问的数据结构，
    也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数
    叫做散列函数，存放记录的数组叫做散列表。
    ",
    "6":"
    AR模型是一种线性预测模型，即已知N个数据，可由模型推出第N点前面或后面的数据
    MA模型(moving average model)滑动平均模型，模型参量法谱分析方法之一
    ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率
    分析方法之一。是研究平稳随机过程有理谱的典型方法，比上述两种有较优良的谱分辨率性能，但
    其参数估算比较繁琐
    GARCH模型，称为广义ARCH模型，适用于波动性的分析和预测。
    ",
    "7":"
    卡特兰数又称卡塔兰数，Catalan number
    从2n位二进制数中填入n个1的方案数为c(2n,n)，不填1的其余n位自动填0。从中减去不符合要求
    （从左而右扫描，0的累计数大于1的累计数）的方案数即为所求
    C(n+1) = C(0)C(n) + C(1)C(n-1) + ... + C(n)C(0)
    (n-3)C(n) = (n/2)(C(3)C(n-1) + C(4)C(n-2) + C(5)C(n-3) + ... + C(n-2)C(4) + C(n-1)C(3))
    h(n) = C(2n,n)/(n+1) (n=0,1,2,...)
    h(n) = C(2n,n) - C(2n,n-1) (n=0,1,2,...)
    0 1
    1 1
    ",
    "8":"
    基尼系数，分问题中假设有K个类，样本点属于第K个类的概率为pk，则概率分布的基尼指数定义为
    Gini(p) = 求和(pk(1-pk)) = 1 - 求和(pk**2)
    特性
    类别个数越少，基尼系数越低
    类别个数相同时，类别集中度越高，基尼系数越低
    当类别越少，类别集中度越高的时候，基尼系数越低；当类别越多，类别集中度越低的时候，基尼系数
    越高
    熵H(X) = -求和(pi*log(pi))
    特性
    非负性，信息量为正值
    对称性，关于P=0.5对称
    确定性，0或1时是确定状态
    极值性，0.5时取到极值-
    ",
    "线性回归":"
    线性回归的基本假设：
    1. 随机误差项是一个期望值或平均值为0的随机变量
    2. 对于解释变量的所有观测值，随机误差项有相同的方差
    3. 随机误差项彼此不相关
    4. 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立
    5. 解释变量之间不存在精确的(完全的)线性关系，即解释变量的样本观测值矩阵是满秩矩阵
    6. 随机误差项服从正态分布

    当存在异方差时，普通最小二乘法估计存在以下问题：参数估计值虽然是无偏的，但不是最小方差
    线性无偏估计

    杜宾-瓦特森(DW)检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。

    所谓多重共线性(Multicollinearity)是指线性回归模型中的解释变量之间由于存在精确相关关系
    或高度相关关系而使模型估计失真或难以估计准确。影响如下
    1. 完全共线性下参数估计量不存在
    2. 近似共线性下OLS估计量非有效，多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀
    因子
    3. 参数估计量经济含义不合理
    4. 变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外
    5. 模型的预测功能失效。变大的方差容易是区间预测的“区间”变大，使预测失去意义

    ",
    "LASSO 与 Ridge":"
    LASSO通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，即强制系数
    绝对值之和小于某个固定值；同时设定一些回归系数为零。因此保留了子集收缩的优点，是一种
    处理具有复共线性数据的有偏估计

    ",
    "PDF,PMF,CDF":"
    概率质量函数(probability mass function,PMF)是离散随机变量在各特定取值上的概率
    概率密度函数(probability density function,PDF)是对连续随机变量定义的，本身不是概率
    只有对连续随机变量的取值进行积分后才是概率
    累积分布函数(cumulative distribution function,CDF)能完整描述一个实数随机变量X的概率
    分布，是概率密度函数的积分
    ",
    "分词方法":"
    1. 基于语法和规则的分词法
    2. 机械式分词法(基于词典)：最大匹配法（正向、逆向）、最少切分法（使一句中切分出的词数
    最小，这也是基于词典分词的方法）
    3. 基于统计的方法，根据字符串在语料库中出现的统计频率来决定其是否构成词。条件随机场是
    一个基于统计的序列标记和分割的方法，定义了整个标签序列的联合概率。

    ",
    "序列标注":"
    隐马尔可夫模型(Hidden Markov Model, HMM)
    最大熵马尔可夫模型(Maximum Entropy Markov Model, MEMM)
    条件随机场(Conditional Random Field, CRF)
    HMM，对转移概率和表现概率直接建模，统计共现概率
    MEMM，对转移概率和表现概率建立联合概率，统计时统计的是条件概率，容易陷入局部最优
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，计算全局最优输出节点
    的条件概率，特征设计灵活，同时需要训练的参数更多，存在训练代价大、复杂度高的缺点。
    ",
    "特征提取":"
    K-L变换是除了PCA外的另一种常用的特征提取方法，它有很多种形式，最基本的形式跟PCA类似，
    它跟PCA的不同在于，PCA是一种无监督的特征变换，而K-L变换能够考虑到不同的分类信息，实现
    有监督的特征提取
    K-L变换的几个重要性质
    1. 变换后得到的新特征满足零均值
    2. K-L变换是一种正交变换
    3. K-L变换的新特征彼此之间不相关
    4. K-L变换的新特征向量的二阶矩阵是对角阵，且对角线元素就是原特征的二阶矩阵的特征值
    5. K-L变换是信号的最佳压缩表示，用q维新特征表示原样本特征带来的误差在所有q维正交坐标
    变换中最小
    6. 用K-L坐标系来表示元数据，意味着熵最小，即样本的方差信息最大程度的集中在较少的维数上


    K-L变换与PCA的联系与区别
    都属于正交变换
    当对原特征x进行中心化时(即变换矩阵为协方差矩阵)，K-L变换等价于PCA
    PCA是离散K-L变换

    K-L变换可以实现有监督的特征提取，但是PCA的变换是一种无监督的
    在含以上，K-L变换较广义，PCA较狭义
    K-L变换可以处理连续和离散情况，而PCA只针对离散情况
    K-L变换的变换矩阵可以是很多种，如二阶矩阵、协方差矩阵等，而PCA的变换矩阵就是协方差矩
    ",
    "动态规划":"
    动态规划, dynamic programming, DP
    将一个问题拆成几个子问题，分别求解这些子问题，即可推断出大问题的解
    非后效性，未来与过去无关，如果给定某一阶段的状态，则在这一阶段以后过程的发展不受这阶
    段以前各段状态的影响
    最优子结构性质，大问题的最优解可以由小问题的最优解推出，这个性质叫做“最优子结构性质”
    判断是否可以使用DP解决的问题
    能将大问题拆成几个小问题，且满足无后效性、最优子结构性质

    动态规划典型问题
    1. 最大子序列求和
    2. 斐波那契数列
    3. 股票买入卖出
    4.
    "




}
