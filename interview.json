{
  "1":"
  排序算法
  插入排序，每次选择一个元素，并且将这个元素和整个数组中的所有元素进行比较，然后插入到合
  适的位置，时间复杂度O(n^2)

  希尔排序(Shell Sort)，这个是插入排序的修改版，根据步长由长到短分组进行排序，直到步长
  为1为止，希尔排序是一种不稳定的排序算法，对希尔排序的时间复杂度分析很困难

  基数排序(Radix Sort)，基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成
  不同的数字，然后按每个位数分别比较。排序过程是将所有待比较数值统一为同样的数位长度，数
  位较短的数前面补零，然后从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排
  序完成以后，数列就变成一个有序数列。在某些时候，基数排序法的效率高于其它的稳定性排序法
  每次选择两个元素，按照需求进行交换，循环n次(n为总元素个数)，这样小的元素会不断“冒泡”到
  前面来，时间复杂度为O(n^2)

  归并排序(Merge Sort)，归并排序相比较之前的排序算法而言加入了分治法的思想，其算法思路
  如下：
  1. 如果给的数组只有一个元素的话，直接返回(也就是递归到最底层的一个情况)
  2. 把整个数组分为尽可能相等的两个部分
  3. 对于两个被分开的两个部分进行整个归并排序
  4. 把两个被分开且排好序的数组拼接在一起
  算法复杂度nlogn

  堆排序(Heap Sort)，

  桶排序(Bucket Sort)

  快速排序(Quick Sort)

  Bogo排序



  ",
  "2":"
  堆栈
  是一种数据结构，只能在一端对数据项进行插入和删除，先进后出
  栈，由操作系统自动分配释放，存放函数的参数值、局部变量的值
  堆，一般由程序员分配释放
  ",
  "3":"
  等价类
  设R是定义在集合A上的等价关系，与A中一个元素a有关系的所有元素的集合叫做a的等价类
  有效等价类
  是指对于程序的规格说明来说是合理的、有意义的输入数据构成的集合。利用有效等价类可检验
  程序是否实现了规格说明所规定的功能和性能
  无效等价类
  指对程序的规格说明是不合理的或无意义的输入数据所构成的集合，对于具体的问题，无效等价类
  至少应有一个，也可能多个。
  ",
  "4":"
  spearman相关系数，它是衡量两个变量的依赖性的非参数指标，表示独立变量和依赖变量的相关
  方向。如果x增加时，y趋向于增加，spearman系数为正，x增加时，y趋向于减少，spearman系数
  为负。定序变量。
  ",
  "5":"
  散列表(Hash table, 也叫哈希表)，是根据关键码值(Key value)而直接进行访问的数据结构，
  也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数
  叫做散列函数，存放记录的数组叫做散列表。
  ",
  "6":"
  AR模型是一种线性预测模型，即已知N个数据，可由模型推出第N点前面或后面的数据
  MA模型(moving average model)滑动平均模型，模型参量法谱分析方法之一
  ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率
  分析方法之一。是研究平稳随机过程有理谱的典型方法，比上述两种有较优良的谱分辨率性能，但
  其参数估算比较繁琐
  GARCH模型，称为广义ARCH模型，适用于波动性的分析和预测。
  ",
  "7":"
  卡特兰数又称卡塔兰数，Catalan number
  从2n位二进制数中填入n个1的方案数为c(2n,n)，不填1的其余n位自动填0。从中减去不符合要求
  （从左而右扫描，0的累计数大于1的累计数）的方案数即为所求
  C(n+1) = C(0)C(n) + C(1)C(n-1) + ... + C(n)C(0)
  (n-3)C(n) = (n/2)(C(3)C(n-1) + C(4)C(n-2) + C(5)C(n-3) + ... + C(n-2)C(4) + C(n-1)C(3))
  h(n) = C(2n,n)/(n+1) (n=0,1,2,...)
  h(n) = C(2n,n) - C(2n,n-1) (n=0,1,2,...)
  0 1
  1 1
  可以用来处理可能存在的出栈序列
  ",
  "8":"
  基尼系数，分问题中假设有K个类，样本点属于第K个类的概率为pk，则概率分布的基尼指数定义为
  Gini(p) = 求和(pk(1-pk)) = 1 - 求和(pk**2)
  特性
  类别个数越少，基尼系数越低
  类别个数相同时，类别集中度越高，基尼系数越低
  当类别越少，类别集中度越高的时候，基尼系数越低；当类别越多，类别集中度越低的时候，基尼系数
  越高
  熵H(X) = -求和(pi*log(pi))
  特性
  非负性，信息量为正值
  对称性，关于P=0.5对称
  确定性，0或1时是确定状态
  极值性，0.5时取到极值-
  ",
  "线性回归":"
  线性回归的基本假设：
  1. 随机误差项是一个期望值或平均值为0的随机变量
  2. 对于解释变量的所有观测值，随机误差项有相同的方差
  3. 随机误差项彼此不相关
  4. 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立
  5. 解释变量之间不存在精确的(完全的)线性关系，即解释变量的样本观测值矩阵是满秩矩阵
  6. 随机误差项服从正态分布

  当存在异方差时，普通最小二乘法估计存在以下问题：参数估计值虽然是无偏的，但不是最小方差
  线性无偏估计

  杜宾-瓦特森(DW)检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。

  所谓多重共线性(Multicollinearity)是指线性回归模型中的解释变量之间由于存在精确相关关系
  或高度相关关系而使模型估计失真或难以估计准确。影响如下
  1. 完全共线性下参数估计量不存在
  2. 近似共线性下OLS估计量非有效，多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀
  因子
  3. 参数估计量经济含义不合理
  4. 变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外
  5. 模型的预测功能失效。变大的方差容易是区间预测的“区间”变大，使预测失去意义

  ",
  "LASSO 与 Ridge":"
  LASSO通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，即强制系数
  绝对值之和小于某个固定值；同时设定一些回归系数为零。因此保留了子集收缩的优点，是一种
  处理具有复共线性数据的有偏估计

  ",
  "PDF,PMF,CDF":"
  概率质量函数(probability mass function,PMF)是离散随机变量在各特定取值上的概率
  概率密度函数(probability density function,PDF)是对连续随机变量定义的，本身不是概率
  只有对连续随机变量的取值进行积分后才是概率
  累积分布函数(cumulative distribution function,CDF)能完整描述一个实数随机变量X的概率
  分布，是概率密度函数的积分
  ",
  "分词方法":"
  1. 基于语法和规则的分词法
  2. 机械式分词法(基于词典)：最大匹配法（正向、逆向）、最少切分法（使一句中切分出的词数
  最小，这也是基于词典分词的方法）
  3. 基于统计的方法，根据字符串在语料库中出现的统计频率来决定其是否构成词。条件随机场是
  一个基于统计的序列标记和分割的方法，定义了整个标签序列的联合概率。

  ",
  "序列标注":"
  隐马尔可夫模型(Hidden Markov Model, HMM)
  最大熵马尔可夫模型(Maximum Entropy Markov Model, MEMM)
  条件随机场(Conditional Random Field, CRF)
  HMM，对转移概率和表现概率直接建模，统计共现概率
  MEMM，对转移概率和表现概率建立联合概率，统计时统计的是条件概率，容易陷入局部最优
  CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，计算全局最优输出节点
  的条件概率，特征设计灵活，同时需要训练的参数更多，存在训练代价大、复杂度高的缺点。
  ",
  "特征提取":"
  K-L变换是除了PCA外的另一种常用的特征提取方法，它有很多种形式，最基本的形式跟PCA类似，
  它跟PCA的不同在于，PCA是一种无监督的特征变换，而K-L变换能够考虑到不同的分类信息，实现
  有监督的特征提取
  K-L变换的几个重要性质
  1. 变换后得到的新特征满足零均值
  2. K-L变换是一种正交变换
  3. K-L变换的新特征彼此之间不相关
  4. K-L变换的新特征向量的二阶矩阵是对角阵，且对角线元素就是原特征的二阶矩阵的特征值
  5. K-L变换是信号的最佳压缩表示，用q维新特征表示原样本特征带来的误差在所有q维正交坐标
  变换中最小
  6. 用K-L坐标系来表示元数据，意味着熵最小，即样本的方差信息最大程度的集中在较少的维数上


  K-L变换与PCA的联系与区别
  都属于正交变换
  当对原特征x进行中心化时(即变换矩阵为协方差矩阵)，K-L变换等价于PCA
  PCA是离散K-L变换

  K-L变换可以实现有监督的特征提取，但是PCA的变换是一种无监督的
  在含以上，K-L变换较广义，PCA较狭义
  K-L变换可以处理连续和离散情况，而PCA只针对离散情况
  K-L变换的变换矩阵可以是很多种，如二阶矩阵、协方差矩阵等，而PCA的变换矩阵就是协方差矩
  ",
  	"1":"
  	产生式模型(Generative Model)，对于输入x，类别标签y，估计它们的联合概率
  	分布P(x,y)
  	判别式模型估计条件概率分布P(y|x)
  	联系与区别
  	产生式模型可以根据贝叶斯公式得到判别式模型，但反过来不行
  	产生式模型，关注数据是如何生成的
  	判别式模型，关注类别之间的差别
  	常见的判别式模型:
  	Logistic Regression
  	SVM(SVM本身对噪声具有一定的鲁棒性，但实验证明，当噪声率低于一定水平的噪
  	声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低
  	Traditional Neural Networks
  	Nearest Neighbor
  	CRF(Conditional random field, 条件随机场)
  	Linear Discriminant Analysis
  	Boosting
  	Linear Regression
  	常见的产生式模型:
  	Gaussians(混合高斯模型): 混合高斯模型使用K个高斯模型来表征图像中各个像素
  	点的特征，在新一帧图像获得后更新混合高斯模型，用当前图像中的每个像素点与
  	混合高斯模型匹配，如果成功则判定该点为背景点，否则为前景点，主要由方差和
  	均值两个参数决定。
  	Naive Bayes
  	Mixtures of Gaussians
  	Mixtures of Experts
  	HMMs
  	Sigmoidal Belief Networks
  	Bayesian Networks
  	",
  	"2":"
  	置信度：同时购买了商品A和商品B的交易次数/购买了商品A的次数
  	支持度：同时购买了商品A和商品B 的交易次数/总的交易次数
  	",
  	"3":"
  	设A是数域上一个n阶矩阵，若在相同数域上存在另一个n阶矩阵B，使得AB=BA=E
  	则称B是A的逆矩阵
  	伪逆矩阵是逆矩阵的广义形式
  	奇异矩阵的秩不是满秩。
  	可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵

  	伪逆法：径向基(RBF)神经网络的训练算法，解决线性不可分的情况
  	感知器算法：线性分类模型
  	H-K算法：在最小均方误差准则下求得权矢量，二次准则解决非线性问题
  	势函数法：解决非线性问题
  	",
  	"4":"
  	前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率
  	用于评估该序列最匹配的模型
  	Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方
  	法，主要通过EM法迭代实现，用在只有观测序列，无状态序列时
  	维特比算法解决的是给定一个模型和某个特定的输出序列，求最可能产生这个输出
  	的状态序列。
  	极大似然估计，观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数
  	",
  	"5":"
  	在贝斯决策中，对于先验概率p(y)，分为已知和未知两种情况
  	p(y)已知，直接使用贝叶斯公式求后验概率即可
  	p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面
  	而最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的
  	问题
  	",
  	"6":"
  	系数自编码就是用少于输入层神经元数量的隐含层神经元去学习表征输入层的特征
  	相当于把输入层的特征压缩了，所以是特征降维
  	AutoEncoder的结构与神经网络的隐含层相同，由输入L1，输出L2组成，中间则是
  	权重链接，通过L2得到输入的重构L3，最小化L3与L1的差别进行训练得到权重，尽
  	可能的保存L1的信息。
  	AutoEncoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则
  	需要在训练目标函数中加入sparse惩罚项，避免L2直接复制L1（权重全为1），所以
  	称为sparseAutoencoder。

  	LDA(线性判别式)与PCA
  	PCA是一种无监督的数据降维方法，LDA是一种有监督的数据降维方法。PCA和LDA
  	都是将数据投影到新的相互正交的坐标轴上。PCA是将数据投影到方差最大的几个相
  	互正交的方向上，以期待保留最多的样本信息。
  	LDA降维根据以下三个条件
  	1. 尽可能多地保留数据样本的信息
  	2. 寻找是样本尽可能好分的最佳投影方向
  	3. 投影后使得同类样本尽可能近，不同类样本尽可能远
  	",
  	"7":"
  	卷积神经网络计算输出尺寸，卷积层向下取整，弛化层向上取整。
  	",
  	"8":"
  	Fisher准则，根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的
  	法线，向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分
  	开。这种度量通过类内离散矩阵Sw和类间离散矩阵Sb实现
  	感知准则函数，准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通
  	过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感
  	知器的基础
  	支持向量机，基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的
  	间隔为最大，它的基本出发点是使期望泛华风险尽可能小。
  	",
  	"9":"
  	信息增益计算公式，1-信息熵
  	",
  	"10":"
  	logistic回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比
  	与先验概率和似然函数的乘积，logistic回归的输出就是样本属于正类别的几率。
  	是一种广义的线性回归分析模型。
  	logistic函数就是sigmoid函数
  	",
  	"11":"
  	L1正则化/Lasso
  	L1正则化将系数w的1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那
  	些弱的特征所对应的系数变为0，因此L1正则化往往会使学到的模型很稀疏(系数w经常
  	为0)，这个特性使得L1正则化称为一种很好的特征选择方法。
  	L2正则化/Ridge regression
  	L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，
  	这使得L2和L1有着诸多的差异，最明显的一点就是，L2正则化会让系数的取值变得平
  	均。对于关联特征，这意味他们能够获得更相近的对应系数。
  	L2正则化对于特征选择来说是一种稳定的模型，不像L1正则化那样，系数会因为细微
  	的数据变化而波动，所以L2正则化和L1正则化提供的价值不同，L2正则化对于特征理
  	解来说更加有用。
  	",
  	"12":"
  	HK算法思想适用于线性可分和非线性可分的情况，对于线性可分的情况给出最优权矢
  	量，对于非线性可分的情况能够判别出来，以退出迭代过程。
  	",
  	"英文名词":"
  	径向基, Radial basis function
  	EM算法, Expectation maximization
  	",
  	"13":"
  	SGD, stochastic gradient descent, 随机梯度下降
  	BGD, batch gradient descent, 批量梯度下降
  	Adadelta, 自适应学习率调整
  	Momentum,
  	",
  	"14":"
  	黑塞矩阵(Hessian Matrix)，是一个多元函数的二阶偏导数构成的方阵，描述了
  	函数的局部曲率，可以判定多远函数的极值问题。
  	设n元实函数f(x1,x2,...,xn)在点M0(a1,a2,...,an)的邻域内有二阶连续偏导，A为
  	偏导矩阵
  	当A正定矩阵时，函数在该点为极小值
  	负定矩阵，在该点为极大值
  	不定矩阵，不是极值点
  	半正定矩阵或半负定矩阵，可疑极值点

  	埃尔米特矩阵是共轭对称的方阵，矩阵中每一个第i行j列的元素都与第j行i列的元素
  	共轭相等，是实对称矩阵的推广，如果它既不是半正定也不是半负定的，那么称其
  	为不定矩阵

  	鞍点(Saddle point)，在微分方程中沿着某一方向是稳定的，另一条方向是不稳定
  	的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍
  	点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值。

  	正定矩阵，正定矩阵是一种实对称矩阵，对任何非零向量z，都有zT*M*z>0
  	如果是≥0则称为半正定
  	",
  	"15":"
  	克莱姆法则适用于变量和方程数目相等的线性方程组
  	对于矩阵方程AX = B
  	A为系数矩阵
  	B为常数项列向量
  	当B全为零的时候为齐次线性方程组
  	当B不全为零的时候为非齐次线性方程组
  	1. 若系数矩阵可逆(非奇异)，即|A|不等于0，有唯一解，D=A
  	X0 = A(-1)*B
  	xj = Dj/D (j = 1,2,...,n)
  	其中Dj是把D中第j列元素对应地换成常数项而其余各列保持不变所得到的行列式
  	2. n元齐次线性方程组有非零解的充要条件是其系数行列式为零。等价地，方程
  	组有唯一的零解的充要条件是系数矩阵的行列式不为零，其矩阵可逆
  	3. 当方程组的系数行列式不等于零时，方程组有解，且具有唯一的解
  	4. 如果方程组无解或者有两个不同的解，那么方程组的系数行列式必定等于零
  	5. 克莱姆法则不仅仅适用于实数域，它在任何域上面都成立
  	",
  	"16":"
  	快速排序(Quick Sort)
  	时间复杂度并不固定，如果在最坏情况下（元素刚好是反向的）速度比较慢，达到
  	O(n^2)，但是如果在比较理想的情况下时间复杂度为O(nlogn)。
  	元素的选择一般有如下几种
  	永远选择第一个元素
  	永远选择最后一个元素
  	随机选择元素
  	取中间值
  	整个快速排序的核心是分区(paritition)，分区的目的是传入一个数组和选定的一个
  	元素，把所有小于那个元素的其他元素放在左边，大于的放在右边。

  	冒泡排序
  	比较相邻的元素，如果第一个比第二个大，就交换他们两个
  	对每一组相邻元素做同样的工作，从开始第一对到结尾的最后一对
  	针对所有的元素重复以上的步骤，除了第一个
  	持续每次对越来越少的元素重复上面的步骤，知道没有任何一对数字需要比较
  	",
  	"17":"
  	切比雪夫不等式可以使人们在随机变量x分布未知的情况下对事件|X-μ|<e概率做出
  	估计
  	依概率收敛是不确定现象中关于收敛的一种说法
  	Kolmogorov-Smirnov检验(K-S检验)基于累积分布函数，用以检验一个经验分布是
  	否符合某种理论分布或比较两个经验分布是否有显著性差异。
  	T检验，亦称student检验，主要用于样本含量较小，总体标准差δ未知的正态分布，
  	比较两个平均数的差异是否显著，与f检验、卡方检验并列。
  	Wilcoxon符号秩检验，把观测值和零假设的中心位置之差的绝对值的秩分别按照不同
  	的符号相加作为检验统计量，检测成对观测数据之差是否来自均值为0的总体(产生数据
  	的总体是否具有相同的均值)。
  	Q-Q图，用图形的方式比较两个概率分布，把他们的两个分位数放在一起比较，Q代表
  	分位数，如果两个分布相似，则该Q-Q图趋近于落在y=x的线上，要利用QQ图鉴别样本
  	数据是否近似于正态分布，只需看QQ图上的点是否近似地在一条直线附近，而且该直线
  	的斜率为标准差，截距为均值
  	分布的X2拟合优度检验也可以用来检验观测数据是否符合某种分布。
  	对所考察总体中每一个元素同时测定两个指标X，Y，要检验这两个指标是否有关采用
  	列联表的独立性检验
  	",
  	"18":"
  	中心极限定理，是指概率论中讨论随机变量序列部分和分布逐渐近于正态分布的一类
  	定理，伯努利试验中，事件A出现的次数渐进于正态分布的问题。在实际工作中，只要
  	n足够大，便可以把独立同分布的随机变量之和当做正态变量（一个随机变量，以概率
  	取值，它的取值服从正态分布）。
  	大数定律，在随机事件的大量重复出现中，往往呈现几乎必然的规律，这个规律就是
  	大数定律。在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。
  	切比雪夫大数定理
  	伯努利大数定律
  	辛钦大数定律
  	",
  	"19":"
  	偏差(Bias)，描述的是预测值的期望与真实值之间的差距。偏差越大，越偏离真实数据
  	形容数据跟我们期望的中心差得多远，算是有监督的

  	方差(Variance)，描述的是预测值的变化范围，离散程度，也就是其离期望值的距离。
  	方差越大，数据的分布越分散
  	形容数据的分散程度，算是无监督的

  	噪声(noise)，表达了在当前任务上任何学习算法所能达到的期望泛华误差的下届，刻画了
  	学习问题本身的难度

  	对于给定的学习任务，为了取得较好的泛华性能，需使偏差较小，能够充分拟合数据，
  	并且使方差较小，即使得数据扰动产生的影响小。

  	一般来说偏差与方差是有冲突的，这称为偏差-方差窘境，在训练不足时，学习器的拟合能力
  	不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛华错误率；随着
  	训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学习到，
  	方差逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练数据发
  	生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学
  	到了，则将发生过拟合。
  	",
  	"20":"
  	显著性检验的一般步骤
  	1. 提出假设, 根据问题的要求建立原假设H0和对立假设H1
  	2. 选统计量, 根据H0的内容选取一个合适的检验统计量T，确定它的抽样分布，算出抽样分布
  	的分位数
  	3. 给定显著性水平α的值, α一般取得比较小，如0.01，0.05，0.10等
  	4. 确定拒绝域, 在原假设H0正确的条件下，求出能使
  		Pr{(X1,X2,...,Xn)属于W|H0 } ≤ α
  	成立的拒绝域W，拒绝域W与所选的统计量T和假设有关，常用检验统计量T和相应的临界值c
  	表示
  	5. 对H0作出判断, 比较检验统计量T的观测值t和相应的临界值c，看样本观测值是否落入拒绝域
  	如果样本观测值(x1,x2,...,xn)属于W，则拒绝原假设H0，也称为检验结果是显著的；否则不拒绝
  	H0，或称为检验结果是不显著的
  	",
  	"21":"
  	分箱问题
  	无监督分箱
  	等宽分箱
  	将变量的取值范围分为k个等宽的区间，每个区间当做一个分箱
  	 （等深）分箱
  	将观测值按照从小到大的顺序排列，根据观测的个数等分为k部分，每部分当做一个分箱
  	k聚类分箱
  	用k均值聚类法将观测值聚为k类，但在聚类过程中需要保证分箱的有序性。
  	有监督分箱
  	在分箱时考虑因变量的取值，使得分箱后达到最小或最小描述长度(总熵值达到最小)
  	",
  	"22":"
  	数据属性类型
  	1. 标称属性
  	2. 二元属性
  	3. 序数属性
  	4. 数值属性(区间标度属性与比率标度属性)
  	5. 离散属性与连续属性
  	",
  	"23":"
  	截断均值指的是指定0和100之间的百分数p，丢弃高端和低端(p/2)%的数据，然后用常规的方法计算
  	均值，所得的结果就是截断均值",
  	"24":"
  	规范化数据的3种方法
  	最小-最大规范化
  	对原始数据进行线性变换
  	z分数规范化
  	小数定标规范化
  	",
    "1":"
    排序算法
    插入排序，每次选择一个元素，并且将这个元素和整个数组中的所有元素进行比较，然后插入到合
    适的位置，时间复杂度O(n^2)

    希尔排序(Shell Sort)，这个是插入排序的修改版，根据步长由长到短分组进行排序，直到步长
    为1为止，希尔排序是一种不稳定的排序算法，对希尔排序的时间复杂度分析很困难

    基数排序(Radix Sort)，基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成
    不同的数字，然后按每个位数分别比较。排序过程是将所有待比较数值统一为同样的数位长度，数
    位较短的数前面补零，然后从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排
    序完成以后，数列就变成一个有序数列。在某些时候，基数排序法的效率高于其它的稳定性排序法
    每次选择两个元素，按照需求进行交换，循环n次(n为总元素个数)，这样小的元素会不断“冒泡”到
    前面来，时间复杂度为O(n^2)

    归并排序(Merge Sort)，归并排序相比较之前的排序算法而言加入了分治法的思想，其算法思路
    如下：
    1. 如果给的数组只有一个元素的话，直接返回(也就是递归到最底层的一个情况)
    2. 把整个数组分为尽可能相等的两个部分
    3. 对于两个被分开的两个部分进行整个归并排序
    4. 把两个被分开且排好序的数组拼接在一起
    算法复杂度nlogn

    堆排序(Heap Sort)，

    桶排序(Bucket Sort)

    快速排序(Quick Sort)

    Bogo排序



    ",
    "2":"
    堆栈
    是一种数据结构，只能在一端对数据项进行插入和删除，先进后出
    栈，由操作系统自动分配释放，存放函数的参数值、局部变量的值
    堆，一般由程序员分配释放
    ",
    "3":"
    等价类
    设R是定义在集合A上的等价关系，与A中一个元素a有关系的所有元素的集合叫做a的等价类
    有效等价类
    是指对于程序的规格说明来说是合理的、有意义的输入数据构成的集合。利用有效等价类可检验
    程序是否实现了规格说明所规定的功能和性能
    无效等价类
    指对程序的规格说明是不合理的或无意义的输入数据所构成的集合，对于具体的问题，无效等价类
    至少应有一个，也可能多个。
    ",
    "4":"
    spearman相关系数，它是衡量两个变量的依赖性的非参数指标，表示独立变量和依赖变量的相关
    方向。如果x增加时，y趋向于增加，spearman系数为正，x增加时，y趋向于减少，spearman系数
    为负。定序变量。
    ",
    "5":"
    散列表(Hash table, 也叫哈希表)，是根据关键码值(Key value)而直接进行访问的数据结构，
    也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数
    叫做散列函数，存放记录的数组叫做散列表。
    ",
    "6":"
    AR模型是一种线性预测模型，即已知N个数据，可由模型推出第N点前面或后面的数据
    MA模型(moving average model)滑动平均模型，模型参量法谱分析方法之一
    ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率
    分析方法之一。是研究平稳随机过程有理谱的典型方法，比上述两种有较优良的谱分辨率性能，但
    其参数估算比较繁琐
    GARCH模型，称为广义ARCH模型，适用于波动性的分析和预测。
    ",
    "7":"
    卡特兰数又称卡塔兰数，Catalan number
    从2n位二进制数中填入n个1的方案数为c(2n,n)，不填1的其余n位自动填0。从中减去不符合要求
    （从左而右扫描，0的累计数大于1的累计数）的方案数即为所求
    C(n+1) = C(0)C(n) + C(1)C(n-1) + ... + C(n)C(0)
    (n-3)C(n) = (n/2)(C(3)C(n-1) + C(4)C(n-2) + C(5)C(n-3) + ... + C(n-2)C(4) + C(n-1)C(3))
    h(n) = C(2n,n)/(n+1) (n=0,1,2,...)
    h(n) = C(2n,n) - C(2n,n-1) (n=0,1,2,...)
    0 1
    1 1
    ",
    "8":"
    基尼系数，分问题中假设有K个类，样本点属于第K个类的概率为pk，则概率分布的基尼指数定义为
    Gini(p) = 求和(pk(1-pk)) = 1 - 求和(pk**2)
    特性
    类别个数越少，基尼系数越低
    类别个数相同时，类别集中度越高，基尼系数越低
    当类别越少，类别集中度越高的时候，基尼系数越低；当类别越多，类别集中度越低的时候，基尼系数
    越高
    熵H(X) = -求和(pi*log(pi))
    特性
    非负性，信息量为正值
    对称性，关于P=0.5对称
    确定性，0或1时是确定状态
    极值性，0.5时取到极值-
    ",
    "线性回归":"
    线性回归的基本假设：
    1. 随机误差项是一个期望值或平均值为0的随机变量
    2. 对于解释变量的所有观测值，随机误差项有相同的方差
    3. 随机误差项彼此不相关
    4. 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立
    5. 解释变量之间不存在精确的(完全的)线性关系，即解释变量的样本观测值矩阵是满秩矩阵
    6. 随机误差项服从正态分布

    当存在异方差时，普通最小二乘法估计存在以下问题：参数估计值虽然是无偏的，但不是最小方差
    线性无偏估计

    杜宾-瓦特森(DW)检验，计量经济，统计分析中常用的一种检验序列一阶自相关最常用的方法。

    所谓多重共线性(Multicollinearity)是指线性回归模型中的解释变量之间由于存在精确相关关系
    或高度相关关系而使模型估计失真或难以估计准确。影响如下
    1. 完全共线性下参数估计量不存在
    2. 近似共线性下OLS估计量非有效，多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀
    因子
    3. 参数估计量经济含义不合理
    4. 变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外
    5. 模型的预测功能失效。变大的方差容易是区间预测的“区间”变大，使预测失去意义

    ",
    "LASSO 与 Ridge":"
    LASSO通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，即强制系数
    绝对值之和小于某个固定值；同时设定一些回归系数为零。因此保留了子集收缩的优点，是一种
    处理具有复共线性数据的有偏估计

    ",
    "PDF,PMF,CDF":"
    概率质量函数(probability mass function,PMF)是离散随机变量在各特定取值上的概率
    概率密度函数(probability density function,PDF)是对连续随机变量定义的，本身不是概率
    只有对连续随机变量的取值进行积分后才是概率
    累积分布函数(cumulative distribution function,CDF)能完整描述一个实数随机变量X的概率
    分布，是概率密度函数的积分
    ",
    "分词方法":"
    1. 基于语法和规则的分词法
    2. 机械式分词法(基于词典)：最大匹配法（正向、逆向）、最少切分法（使一句中切分出的词数
    最小，这也是基于词典分词的方法）
    3. 基于统计的方法，根据字符串在语料库中出现的统计频率来决定其是否构成词。条件随机场是
    一个基于统计的序列标记和分割的方法，定义了整个标签序列的联合概率。

    ",
    "序列标注":"
    隐马尔可夫模型(Hidden Markov Model, HMM)
    最大熵马尔可夫模型(Maximum Entropy Markov Model, MEMM)
    条件随机场(Conditional Random Field, CRF)
    HMM，对转移概率和表现概率直接建模，统计共现概率
    MEMM，对转移概率和表现概率建立联合概率，统计时统计的是条件概率，容易陷入局部最优
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，计算全局最优输出节点
    的条件概率，特征设计灵活，同时需要训练的参数更多，存在训练代价大、复杂度高的缺点。
    ",
    "特征提取":"
    K-L变换是除了PCA外的另一种常用的特征提取方法，它有很多种形式，最基本的形式跟PCA类似，
    它跟PCA的不同在于，PCA是一种无监督的特征变换，而K-L变换能够考虑到不同的分类信息，实现
    有监督的特征提取
    K-L变换的几个重要性质
    1. 变换后得到的新特征满足零均值
    2. K-L变换是一种正交变换
    3. K-L变换的新特征彼此之间不相关
    4. K-L变换的新特征向量的二阶矩阵是对角阵，且对角线元素就是原特征的二阶矩阵的特征值
    5. K-L变换是信号的最佳压缩表示，用q维新特征表示原样本特征带来的误差在所有q维正交坐标
    变换中最小
    6. 用K-L坐标系来表示元数据，意味着熵最小，即样本的方差信息最大程度的集中在较少的维数上


    K-L变换与PCA的联系与区别
    都属于正交变换
    当对原特征x进行中心化时(即变换矩阵为协方差矩阵)，K-L变换等价于PCA
    PCA是离散K-L变换

    K-L变换可以实现有监督的特征提取，但是PCA的变换是一种无监督的
    在含以上，K-L变换较广义，PCA较狭义
    K-L变换可以处理连续和离散情况，而PCA只针对离散情况
    K-L变换的变换矩阵可以是很多种，如二阶矩阵、协方差矩阵等，而PCA的变换矩阵就是协方差矩
    ",
    "动态规划":"
    动态规划, dynamic programming, DP
    将一个问题拆成几个子问题，分别求解这些子问题，即可推断出大问题的解
    非后效性，未来与过去无关，如果给定某一阶段的状态，则在这一阶段以后过程的发展不受这阶
    段以前各段状态的影响
    最优子结构性质，大问题的最优解可以由小问题的最优解推出，这个性质叫做“最优子结构性质”
    判断是否可以使用DP解决的问题
    能将大问题拆成几个小问题，且满足无后效性、最优子结构性质

    动态规划典型问题
    1. 最大子序列求和
    2. 斐波那契数列
    3. 股票买入卖出
    4.
    ",
    "二叉树遍历问题":"
    一棵非空的二叉树由根节点及左、右子树这三个部分组成，在任一节点上可以按某种次序执行
    以下操作
    1. 访问节点本身(N)
    2. 遍历该节点的左子树(L)
    3. 遍历该节点的右子树(R)
    前序遍历  NLR，访问根节点的操作发生在遍历其左右子树之前
    中序遍历  LNR，访问根节点的操作发生在遍历其左右子树之中
    后序遍历  LRN，访问根节点的操作发生在遍历其左右子树之后
    ",
    "范数":"
    一范数：列范数
    无穷范数：行范数
    二范数：共轭矩阵乘积的最大特征值平方根
    L2正则化，在原来的损失函数基础上加上权重参数的平方和
    L1正则化，在原来的损失函数基础上加上权重参数的绝对值
    ",
    "二叉树":"
    满二叉树，除最后一层无任何子节点外，每一层上的所有节点都有两个子节点二叉树

    完全二叉树，设二叉树的深度为h，除第h层外，其它各层(1~h-1)的结点数都达到最大个数，第
    h层所有的节点都连续集中在最左边，这就是完全二叉树

    平衡二叉搜索树，它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个
    子树都是一棵平衡二叉树。最小二叉平衡树的节点总数计算可以考虑斐波那契数列。

    二叉排序树，左子树上的值均小于或等于根节点的值，右子树上的值均大于或等于根节点的值；
    左、右子树也分别为二叉排序树

    最优二叉树，哈夫曼树，带权路径最短的树
    ",
    "Linux命令":"
    Sort命令，将文本文件内容加以排序，sort可针对文本文件的内容，以行为单位来排序
      -n 依照数值的大小排序
      -r 以相反的顺序来排序
    uniq命令，用于检查及删除文本文件中重复出现的行列
      -c 在每列旁边显示该行重复出现的次数
    head命令，用来显示文件的开头至标准输出中
      -n 显示每个文件的前k行内容

    查看本系统支持的shell类型配置文件
        /etc/shells
    bash环境中挂起当前进程的方式
        ctrl+z
    cat命令用于连接文件并打印在标准输出设备上
    top命令用于实时显示process的动态
    grep命令用于查找文件里符合条件的字符串，如果发现某文件的内容符合所指定的范本样式，
    预设grep指令会把含有范本样式的那一列显示出来
    find命令用来在指定目录下查找文件。任何未予参数值钱的字符串都将被视为欲查找的目录名。
    如果使用该命令，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查
    找到的子目录和文件全部进行显示。
        find path -option
            -name，-iname，文件名称符合name 的文件。iname会忽略大小写

    ",
    "进程状态":"
    三态模型
      运行态(running)
      就绪态(ready)
      等待态(wait)
      运行态-等待态
      等待态-就绪态
      运行态-就绪态
      就绪态-运行态
    五态模型
      运行态
      就绪态
      等待态
      新建态
      终止态
      NULL-新建态
      新建态-就绪态
      运行态-终止态
      终止态-NULL
      就绪态-终止态
      等待态-终止态
    ",
    "线程与进程":"
    进程，是执行中的一段程序，一旦程序被载入到内存中并准备执行，它就是一个进程。进程是
    表示资源分配的基本概念，又是调度运行的基本单位，是系统中的并发执行单位
    线程，单个进程中执行中每个任务就是一个线程。线程是进程中执行运算的最小单位
    2、一个线程只能属于一个进程，但是一个进程可以拥有多个线程。多线程处理就是允许一个进程中在同一时刻执行多个任务。


    3、线程是一种轻量级的进程，与进程相比，线程给操作系统带来侧创建、维护、和管理的负担要轻，意味着线程的代价或开销比较小。


    4、线程没有地址空间，线程包含在进程的地址空间中。线程上下文只包含一个堆栈、一个寄存器、一个优先权，线程文本包含在他的进程 的文本片段中，进程拥有的所有资源都属于线程。所有的线程共享进程的内存和资源。 同一进程中的多个线程共享代码段(代码和常量)，数据段(全局变量和静态变量)，扩展段(堆存储)。但是每个线程拥有自己的栈段， 寄存器的内容，栈段又叫运行时段，用来存放所有局部变量和临时变量。


    5、父和子进程使用进程间通信机制，同一进程的线程通过读取和写入数据到进程变量来通信。


    6、进程内的任何线程都被看做是同位体，且处于相同的级别。不管是哪个线程创建了哪一个线程，进程内的任何线程都可以销毁、挂起、恢复和更改其它线程的优先权。线程也要对进程施加控制，进程中任何线程都可以通过销毁主线程来销毁进程，销毁主线程将导致该进程的销毁，对主线程的修改可能影响所有的线程。


    7、子进程不对任何其他子进程施加控制，进程的线程可以对同一进程的其它线程施加控制。子进程不能对父进程施加控制，进程中所有线程都可以对主线程施加控制。


    相同点：


    进程和线程都有ID/寄存器组、状态和优先权、信息块，创建后都可更改自己的属性，都可与父进程共享资源、都不鞥直接访问其他无关进程或线程的资源。
    ",
    "统计学知识":"
    当二项分布的n很大时候，二项分布可以用高斯分布逼近，但不能用泊松分布逼近

    实数集上的凸函数，一般的判别方法是求它的二阶导数，如果其二阶导数在区间上非负，就称为
    凸函数，如果其二阶导数在区间上恒大于0，就称为严格凸函数

    凹函数反之

    二元函数的凹凸性用海塞矩阵来判断，根据海塞矩阵的正定性判断凹凸性

    ",
    "推荐算法分类":"
    基于内容的推荐算法
      用户喜欢和自己关注过的item在内容上类似的item，这种方法可以避免item的。问题(
      冷启动：如果一个item从没有被关注过，其他推荐算法则很少会去推荐，但是基于内容的推荐
      算法可以分析item之间的关系，实现推荐)。
      弊端在于推荐的item可能会重复，典型的就是新闻推荐，如果你看了一则关于MH370的新闻，
      很可能推荐的新闻和你浏览过的内容一致；
      另外一个弊端则是对于一些多媒体的推荐(比如说音乐、电影、图片)，由于很难提取内容特征
      则很难进行推荐，一种解决方式是人工给这些item打标签

    协同过滤推荐算法
      原理是用户喜欢那些具有相似兴趣的用户喜欢过的商品，有两种
        基于用户的协同过滤算法(user-based collaborative filtering)
          基于用户的协同过滤通过不同用户对物品的评分来评测用户之间的相似性，基于用户的相
          似性做推荐，简单的讲就是给用户推荐和他兴趣相投的其他用户喜欢的物品
        基于item的协同过滤算法(item-based collaborative filtering)
          基于item的协同过滤通过不同用户对不同item的评分来评测item之间的相似性，基于item
          的相似性做推荐，简单的讲就是给用户推荐和他之前喜欢物品相似的物品
        混合推荐算法主体思路还是基于用户的协同过滤，只是在计算两个用户的相似度时又嵌套了
        item-based思想
      这几种方法都是将用户的所有数据读入到内存中进行运算的，因此属于memory-based filtering
      另一种则是model-based collaborative filtering，包括aspect model，plsa，lda，
      聚类，svd，matrix factorization等，这种方法训练过程比较长，但是训练完成后，推荐
      过程比较快
    基于知识的推荐算法
      最后一种是基于知识的推荐算法，也有人将这种方法归为基于内容的推荐，这种方法比较典型
      的就是构建领域本体，或者是建立一定的规则，进行推荐。
    ",
    "SQL性能优化":"
    sql需要避免在索引字段上使用函数
    避免在where句中使用in，not in，可以使用exist和not exist代替
    将对于同一个表格的多个字段的操作写到同一个sql中，而不是分开成两个sql语句实现
    避免建立索引的列中使用空值

    mysql中
    change可以只改变列名称
    也可以只改变列数据类型
    还可以同时改变列名称和列数据类型
    modify只能用来改变列的数据类型

    ",
    "顺序查找":"
    顺序查找方式来确定元素所在的块，平均查找长度为:(b+1)/2+(s+1)/2
    b表示索引表的长度(即总块数),s代表块内所含元素的个数
    ",
    "贝叶斯信念网络":"
    贝叶斯网络比朴素贝叶斯更复杂，而想构造和训练出一个好的贝叶斯网络更是异常艰难。但是贝
    叶斯网络是模拟人的认知思维推理模式，用一组条件概率函数以及有向无环图对不确定性的因果
    推理关系建模，因此具有更高的实用价值
    贝叶斯网络最强大之处在于从每个阶段结果所获得的概率都是数学与科学的反映(例如，贝叶斯
    网络返回的不是类标号，而使返回概率分布，给出每个类的概率)，换句话说，假设我们了解了
    足够多的信息，根据这些信息获继而得统计知识，网络就会告诉我们合理的推断
    贝叶斯网络很容易扩展，以适应不断变化的需求和变化的知识
    ",
    "K均值":"
    邻近度函数           质心
    曼哈顿距离           中位数
    平方欧几里德距离      均值
    余弦距离              均值
    Bregman散度         均值
    ",
    "频繁项集":"
    项集，最基本的模式是项集，它是指若干个项的集合
    频繁模式，数据集中频繁出现的项集、序列或子结构
    频繁项集，支持度大于等于最小支持度的集合
    支持度，某个集合在所有事物中出现的频率
    Apriori原理，如果某个项集是频繁的，它的所有子集也是频繁的，如果一个项集是非频繁集，
    那么它的所有超集也非频繁的
    ",
    "网络协议":"
    1. ping后面跟的是地址，需要先将域名转换成ip地址，用到了DNS
    2. 获取到ip地址之后，在数据链路层是根据MAC地址传输的，所以要用到ARP解析服务，获取到
    MAC地址
    3. ping功能是测试另一台主机是否可达，程序发送一份ICMP回显请求给目标主机，并等待返回
    ICMP回显应答(ICMP主要是用于ip主机、路由器之间传递控制信息，控制信息是指网络通不通，
    主机是否可达)
    4. TCP不涉及数据传输不会用到
    5. TCP是面向连接的，如打电话要先拨号建立连接
    6. TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流
    7. UDP是无连接的，即发送数据之前不需要建立连接
    ",
    "筛法公式":"
    f(2) = 1
    f(3) = 2
    f(4) = 9
    f(5) = 44
    ",
    "C++":"
    构造函数在以下情况会被调用
      使用一个类的对象去初始化该类的一个新对象
      被调用函数的形参是类的对象
      当函数的返回值是类的对象时，函数执行完成返回调用者
    代码重用方式
      继承、多态、模板
    ",
    "设计模式":"
    面向接口编程
    优先使用对象组合
    ",
    "java":"
    java只支持单重继承，一个类可以实现多个接口
    ",
    "正则表达式":"
    ^ 匹配字符串的开头
    $ 匹配字符串的结尾
    ? 匹配前面的子表达式零次或一次
    * 匹配前面的子表达式零次或多次
    + 匹配前面的子表达式一次或多次
    ",
    "抽样方法":"
    随机抽样
        随机抽样要求严格遵循概率原则，每个抽样单元被抽中的概率相同，并且可以重现。随机
        抽样常常用于总体个数较少时，它的主要特征是从总体中逐个抽取。
    分层抽样
        分层抽样是指在抽样时，将总体分成互不相交的层，然后按照一定的比例，从各层独立地
        抽取一定数量的个体，将各层取出的个体合在一起作为样本的方法。层内变异越小越好，
        层间变异越大越好

        分层以后在每一层进行简单随机抽样
          等数分配法，对每一层都分配同样的个体数
          等比分配法，让每一层抽得的个体数与该类总体的个体数之比都相同
          最优分配法，即各层抽得的样本数与所抽得的总样本数之比等于该层方差与各类方差之和
          的比
        优点
          减小抽样误差，分层后增加了层内的同质性，因而可使观察值的变异度减小，各层的抽样
          误差减小。在样本含量相同的情况下，分层抽样总的标准误一般均小于单纯随机抽样、系
          统抽样和整群抽样的标准误
          抽样方法灵活，可以根据各层的具体情况对不同的层采用不同的抽样方法
          可对不同层独立进行分析
        缺点
          若分层变量选择不当，层内变异变大，层间均数相近，分层抽样就失去了意义
    整体抽样
    整群抽样
        又称聚类抽样，是将总体中各单位归并成若干个互不交叉、互不重复的集合，称之为群；然
        后以群为抽样单位抽取样本的一种抽样方式。应用整群抽样时，要求各群有较好的代表性，
        即群内各单位的差异要大，群间差异要小
        优点
          整群抽样的优点是实施方便、节省经费
        缺点
          往往由于不同群之间的差异较大，由此引起的抽样误差往往大于简单随机抽样
        与分层抽样的区别
          1. 分层抽样要求各层之间的差异很大，层内个体或单元差异小，而整群抽样要求群与群之间
          的差异比较小，区内个体或单元差异大
          2. 分层抽样的样本是从每个层内抽取若干单元或个体构成，而整群抽样则是要么整群抽取
          要么整群不被抽取

    系统抽样
        当总体中的个体数较多时，可将总体分为均衡的几个部分，按照预先定出的规则，从每一部分
        抽取一个个体，得到所需样本
    ",
    ""





}
